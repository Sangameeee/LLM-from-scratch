{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5o6YYdByd2JXGxRPbEpuS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sangameeee/LLM-from-scratch/blob/main/Bpetokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V40OZ-MEBKHs"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, deque\n",
        "from functools import lru_cache\n",
        "import json\n",
        "\n",
        "\n",
        "class BPETokenizerSimple:\n",
        "    def __init__(self):\n",
        "        # Maps token_id to token_str (e.g., {11246: \"some\"})\n",
        "        self.vocab = {}\n",
        "        # Maps token_str to token_id (e.g., {\"some\": 11246})\n",
        "        self.inverse_vocab = {}\n",
        "        # Dictionary of BPE merges: {(token_id1, token_id2): merged_token_id}\n",
        "        self.bpe_merges = {}\n",
        "\n",
        "        # For the official OpenAI GPT-2 merges, use a rank dict:\n",
        "        #  of form {(string_A, string_B): rank}, where lower rank = higher priority\n",
        "        self.bpe_ranks = {}\n",
        "\n",
        "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
        "        \"\"\"\n",
        "        Train the BPE tokenizer from scratch.\n",
        "\n",
        "        Args:\n",
        "            text (str): The training text.\n",
        "            vocab_size (int): The desired vocabulary size.\n",
        "            allowed_special (set): A set of special tokens to include.\n",
        "        \"\"\"\n",
        "\n",
        "        # Preprocess: Replace spaces with \"Ġ\"\n",
        "        # Note that Ġ is a particularity of the GPT-2 BPE implementation\n",
        "        # E.g., \"Hello world\" might be tokenized as [\"Hello\", \"Ġworld\"]\n",
        "        # (GPT-4 BPE would tokenize it as [\"Hello\", \" world\"])\n",
        "        processed_text = []\n",
        "        for i, char in enumerate(text):\n",
        "            if char == \" \" and i != 0:\n",
        "                processed_text.append(\"Ġ\")\n",
        "            if char != \" \":\n",
        "                processed_text.append(char)\n",
        "        processed_text = \"\".join(processed_text)\n",
        "\n",
        "        # Initialize vocab with unique characters, including \"Ġ\" if present\n",
        "        # Start with the first 256 ASCII characters\n",
        "        unique_chars = [chr(i) for i in range(256)]\n",
        "        unique_chars.extend(\n",
        "            char for char in sorted(set(processed_text))\n",
        "            if char not in unique_chars\n",
        "        )\n",
        "        if \"Ġ\" not in unique_chars:\n",
        "            unique_chars.append(\"Ġ\")\n",
        "\n",
        "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
        "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
        "\n",
        "        # Add allowed special tokens\n",
        "        if allowed_special:\n",
        "            for token in allowed_special:\n",
        "                if token not in self.inverse_vocab:\n",
        "                    new_id = len(self.vocab)\n",
        "                    self.vocab[new_id] = token\n",
        "                    self.inverse_vocab[token] = new_id\n",
        "\n",
        "        # Tokenize the processed_text into token IDs\n",
        "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
        "\n",
        "        # BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
        "        for new_id in range(len(self.vocab), vocab_size):\n",
        "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
        "            if pair_id is None:\n",
        "                break\n",
        "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
        "            self.bpe_merges[pair_id] = new_id\n",
        "\n",
        "        # Build the vocabulary with merged tokens\n",
        "        for (p0, p1), new_id in self.bpe_merges.items():\n",
        "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
        "            self.vocab[new_id] = merged_token\n",
        "            self.inverse_vocab[merged_token] = new_id\n",
        "\n",
        "    def load_vocab_and_merges_from_openai(self, vocab_path, bpe_merges_path):\n",
        "        \"\"\"\n",
        "        Load pre-trained vocabulary and BPE merges from OpenAI's GPT-2 files.\n",
        "\n",
        "        Args:\n",
        "            vocab_path (str): Path to the vocab file (GPT-2 calls it 'encoder.json').\n",
        "            bpe_merges_path (str): Path to the bpe_merges file  (GPT-2 calls it 'vocab.bpe').\n",
        "        \"\"\"\n",
        "        # Load vocabulary\n",
        "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            loaded_vocab = json.load(file)\n",
        "            # Convert loaded vocabulary to correct format\n",
        "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}\n",
        "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}\n",
        "\n",
        "        # Handle newline character without adding a new token\n",
        "        if \"\\n\" not in self.inverse_vocab:\n",
        "            # Use an existing token ID as a placeholder for '\\n'\n",
        "            # Preferentially use \"<|endoftext|>\" if available\n",
        "            fallback_token = next((token for token in [\"<|endoftext|>\", \"Ġ\", \"\"] if token in self.inverse_vocab), None)\n",
        "            if fallback_token is not None:\n",
        "                newline_token_id = self.inverse_vocab[fallback_token]\n",
        "            else:\n",
        "                # If no fallback token is available, raise an error\n",
        "                raise KeyError(\"No suitable token found in vocabulary to map '\\\\n'.\")\n",
        "\n",
        "            self.inverse_vocab[\"\\n\"] = newline_token_id\n",
        "            self.vocab[newline_token_id] = \"\\n\"\n",
        "\n",
        "        # Load GPT-2 merges and store them with an assigned \"rank\"\n",
        "        self.bpe_ranks = {}  # reset ranks\n",
        "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            lines = file.readlines()\n",
        "            if lines and lines[0].startswith(\"#\"):\n",
        "                lines = lines[1:]\n",
        "\n",
        "            rank = 0\n",
        "            for line in lines:\n",
        "                pair = tuple(line.strip().split())\n",
        "                if len(pair) == 2:\n",
        "                    token1, token2 = pair\n",
        "                    # If token1 or token2 not in vocab, skip\n",
        "                    if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
        "                        self.bpe_ranks[(token1, token2)] = rank\n",
        "                        rank += 1\n",
        "                    else:\n",
        "                        print(f\"Skipping pair {pair} as one token is not in the vocabulary.\")\n",
        "\n",
        "    def encode(self, text, allowed_special=None):\n",
        "        \"\"\"\n",
        "        Encode the input text into a list of token IDs, with tiktoken-style handling of special tokens.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input text to encode.\n",
        "            allowed_special (set or None): Special tokens to allow passthrough. If None, special handling is disabled.\n",
        "\n",
        "        Returns:\n",
        "            List of token IDs.\n",
        "        \"\"\"\n",
        "        import re\n",
        "\n",
        "        token_ids = []\n",
        "\n",
        "        # If special token handling is enabled\n",
        "        if allowed_special is not None and len(allowed_special) > 0:\n",
        "            # Build regex to match allowed special tokens\n",
        "            special_pattern = (\n",
        "                \"(\" + \"|\".join(re.escape(tok) for tok in sorted(allowed_special, key=len, reverse=True)) + \")\"\n",
        "            )\n",
        "\n",
        "            last_index = 0\n",
        "            for match in re.finditer(special_pattern, text):\n",
        "                prefix = text[last_index:match.start()]\n",
        "                token_ids.extend(self.encode(prefix, allowed_special=None))  # Encode prefix without special handling\n",
        "\n",
        "                special_token = match.group(0)\n",
        "                if special_token in self.inverse_vocab:\n",
        "                    token_ids.append(self.inverse_vocab[special_token])\n",
        "                else:\n",
        "                    raise ValueError(f\"Special token {special_token} not found in vocabulary.\")\n",
        "                last_index = match.end()\n",
        "\n",
        "            text = text[last_index:]  # Remaining part to process normally\n",
        "\n",
        "            # Check if any disallowed special tokens are in the remainder\n",
        "            disallowed = [\n",
        "                tok for tok in self.inverse_vocab\n",
        "                if tok.startswith(\"<|\") and tok.endswith(\"|>\") and tok in text and tok not in allowed_special\n",
        "            ]\n",
        "            if disallowed:\n",
        "                raise ValueError(f\"Disallowed special tokens encountered in text: {disallowed}\")\n",
        "\n",
        "        # If no special tokens, or remaining text after special token split:\n",
        "        tokens = []\n",
        "        lines = text.split(\"\\n\")\n",
        "        for i, line in enumerate(lines):\n",
        "            if i > 0:\n",
        "                tokens.append(\"\\n\")\n",
        "            words = line.split()\n",
        "            for j, word in enumerate(words):\n",
        "                if j == 0 and i > 0:\n",
        "                    tokens.append(\"Ġ\" + word)\n",
        "                elif j == 0:\n",
        "                    tokens.append(word)\n",
        "                else:\n",
        "                    tokens.append(\"Ġ\" + word)\n",
        "\n",
        "        for token in tokens:\n",
        "            if token in self.inverse_vocab:\n",
        "                token_ids.append(self.inverse_vocab[token])\n",
        "            else:\n",
        "                token_ids.extend(self.tokenize_with_bpe(token))\n",
        "\n",
        "        return token_ids\n",
        "\n",
        "    def tokenize_with_bpe(self, token):\n",
        "        \"\"\"\n",
        "        Tokenize a single token using BPE merges.\n",
        "\n",
        "        Args:\n",
        "            token (str): The token to tokenize.\n",
        "\n",
        "        Returns:\n",
        "            List[int]: The list of token IDs after applying BPE.\n",
        "        \"\"\"\n",
        "        # Tokenize the token into individual characters (as initial token IDs)\n",
        "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
        "        if None in token_ids:\n",
        "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
        "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
        "\n",
        "        # If we haven't loaded OpenAI's GPT-2 merges, use my approach\n",
        "        if not self.bpe_ranks:\n",
        "            can_merge = True\n",
        "            while can_merge and len(token_ids) > 1:\n",
        "                can_merge = False\n",
        "                new_tokens = []\n",
        "                i = 0\n",
        "                while i < len(token_ids) - 1:\n",
        "                    pair = (token_ids[i], token_ids[i + 1])\n",
        "                    if pair in self.bpe_merges:\n",
        "                        merged_token_id = self.bpe_merges[pair]\n",
        "                        new_tokens.append(merged_token_id)\n",
        "                        # Uncomment for educational purposes:\n",
        "                        # print(f\"Merged pair {pair} -> {merged_token_id} ('{self.vocab[merged_token_id]}')\")\n",
        "                        i += 2  # Skip the next token as it's merged\n",
        "                        can_merge = True\n",
        "                    else:\n",
        "                        new_tokens.append(token_ids[i])\n",
        "                        i += 1\n",
        "                if i < len(token_ids):\n",
        "                    new_tokens.append(token_ids[i])\n",
        "                token_ids = new_tokens\n",
        "            return token_ids\n",
        "\n",
        "        # Otherwise, do GPT-2-style merging with the ranks:\n",
        "        # 1) Convert token_ids back to string \"symbols\" for each ID\n",
        "        symbols = [self.vocab[id_num] for id_num in token_ids]\n",
        "\n",
        "        # Repeatedly merge all occurrences of the lowest-rank pair\n",
        "        while True:\n",
        "            # Collect all adjacent pairs\n",
        "            pairs = set(zip(symbols, symbols[1:]))\n",
        "            if not pairs:\n",
        "                break\n",
        "\n",
        "            # Find the pair with the best (lowest) rank\n",
        "            min_rank = float(\"inf\")\n",
        "            bigram = None\n",
        "            for p in pairs:\n",
        "                r = self.bpe_ranks.get(p, float(\"inf\"))\n",
        "                if r < min_rank:\n",
        "                    min_rank = r\n",
        "                    bigram = p\n",
        "\n",
        "            # If no valid ranked pair is present, we're done\n",
        "            if bigram is None or bigram not in self.bpe_ranks:\n",
        "                break\n",
        "\n",
        "            # Merge all occurrences of that pair\n",
        "            first, second = bigram\n",
        "            new_symbols = []\n",
        "            i = 0\n",
        "            while i < len(symbols):\n",
        "                # If we see (first, second) at position i, merge them\n",
        "                if i < len(symbols) - 1 and symbols[i] == first and symbols[i+1] == second:\n",
        "                    new_symbols.append(first + second)  # merged symbol\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_symbols.append(symbols[i])\n",
        "                    i += 1\n",
        "            symbols = new_symbols\n",
        "\n",
        "            if len(symbols) == 1:\n",
        "                break\n",
        "\n",
        "        # Finally, convert merged symbols back to IDs\n",
        "        merged_ids = [self.inverse_vocab[sym] for sym in symbols]\n",
        "        return merged_ids\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        \"\"\"\n",
        "        Decode a list of token IDs back into a string.\n",
        "\n",
        "        Args:\n",
        "            token_ids (List[int]): The list of token IDs to decode.\n",
        "\n",
        "        Returns:\n",
        "            str: The decoded string.\n",
        "        \"\"\"\n",
        "        decoded_string = \"\"\n",
        "        for i, token_id in enumerate(token_ids):\n",
        "            if token_id not in self.vocab:\n",
        "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
        "            token = self.vocab[token_id]\n",
        "            if token == \"\\n\":\n",
        "                if decoded_string and not decoded_string.endswith(\" \"):\n",
        "                    decoded_string += \" \"  # Add space if not present before a newline\n",
        "                decoded_string += token\n",
        "            elif token.startswith(\"Ġ\"):\n",
        "                decoded_string += \" \" + token[1:]\n",
        "            else:\n",
        "                decoded_string += token\n",
        "        return decoded_string\n",
        "\n",
        "    def save_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
        "        \"\"\"\n",
        "        Save the vocabulary and BPE merges to JSON files.\n",
        "\n",
        "        Args:\n",
        "            vocab_path (str): Path to save the vocabulary.\n",
        "            bpe_merges_path (str): Path to save the BPE merges.\n",
        "        \"\"\"\n",
        "        # Save vocabulary\n",
        "        with open(vocab_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            json.dump(self.vocab, file, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # Save BPE merges as a list of dictionaries\n",
        "        with open(bpe_merges_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            merges_list = [{\"pair\": list(pair), \"new_id\": new_id}\n",
        "                           for pair, new_id in self.bpe_merges.items()]\n",
        "            json.dump(merges_list, file, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def load_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
        "        \"\"\"\n",
        "        Load the vocabulary and BPE merges from JSON files.\n",
        "\n",
        "        Args:\n",
        "            vocab_path (str): Path to the vocabulary file.\n",
        "            bpe_merges_path (str): Path to the BPE merges file.\n",
        "        \"\"\"\n",
        "        # Load vocabulary\n",
        "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            loaded_vocab = json.load(file)\n",
        "            self.vocab = {int(k): v for k, v in loaded_vocab.items()}\n",
        "            self.inverse_vocab = {v: int(k) for k, v in loaded_vocab.items()}\n",
        "\n",
        "        # Load BPE merges\n",
        "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            merges_list = json.load(file)\n",
        "            for merge in merges_list:\n",
        "                pair = tuple(merge[\"pair\"])\n",
        "                new_id = merge[\"new_id\"]\n",
        "                self.bpe_merges[pair] = new_id\n",
        "\n",
        "    @lru_cache(maxsize=None)\n",
        "    def get_special_token_id(self, token):\n",
        "        return self.inverse_vocab.get(token, None)\n",
        "\n",
        "    @staticmethod\n",
        "    def find_freq_pair(token_ids, mode=\"most\"):\n",
        "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
        "\n",
        "        if not pairs:\n",
        "            return None\n",
        "\n",
        "        if mode == \"most\":\n",
        "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
        "        elif mode == \"least\":\n",
        "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
        "        else:\n",
        "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
        "\n",
        "    @staticmethod\n",
        "    def replace_pair(token_ids, pair_id, new_id):\n",
        "        dq = deque(token_ids)\n",
        "        replaced = []\n",
        "\n",
        "        while dq:\n",
        "            current = dq.popleft()\n",
        "            if dq and (current, dq[0]) == pair_id:\n",
        "                replaced.append(new_id)\n",
        "                # Remove the 2nd token of the pair, 1st was already removed\n",
        "                dq.popleft()\n",
        "            else:\n",
        "                replaced.append(current)\n",
        "\n",
        "        return replaced"
      ]
    }
  ]
}